{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F2RjUhNsqY4L"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "data_path = \"model/human_text.txt\"\n",
    "data_path2 = \"model/robot_text.txt\"\n",
    "# Defining lines as a list of each line\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "  lines = f.read().split('\\n')\n",
    "with open(data_path2, 'r', encoding='utf-8') as f:\n",
    "  lines2 = f.read().split('\\n')\n",
    "lines = [re.sub(r\"\\[\\w+\\]\",'hola',line.lower()) for line in lines]\n",
    "lines = [\" \".join(re.findall(r\"\\w+\",line.lower())) for line in lines]\n",
    "lines2 = [re.sub(r\"\\[\\w+\\]\",'',line.lower()) for line in lines2]\n",
    "lines2 = [\" \".join(re.findall(r\"\\w+\",line.lower())) for line in lines2]\n",
    "# Grouping lines by response pair\n",
    "pairs = list(zip(lines,lines2))\n",
    "#random.shuffle(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "bw7iFyTEFriu",
    "outputId": "d10fd3a9-023e-4850-ef35-2dbac08098a0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_docs = []\n",
    "target_docs = []\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "for line in pairs:\n",
    "  input_doc, target_doc = line[0], line[1]\n",
    "  # Appending each input sentence to input_docs\n",
    "  input_docs.append(input_doc)\n",
    "  # Splitting words from punctuation  \n",
    "  target_doc = \" \".join(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc))\n",
    "  # Redefine target_doc below and append it to target_docs\n",
    "  target_doc = '<START> ' + target_doc + ' <END>'\n",
    "  target_docs.append(target_doc)\n",
    "  \n",
    "  # Now we split up each sentence into words and add each unique word to our vocabulary set\n",
    "  for token in re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc):\n",
    "    if token not in input_tokens:\n",
    "      input_tokens.add(token)\n",
    "  for token in target_doc.split():\n",
    "    if token not in target_tokens:\n",
    "      target_tokens.add(token)\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "\n",
    "input_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(input_tokens)])\n",
    "target_features_dict = dict(\n",
    "    [(token, i) for i, token in enumerate(target_tokens)])\n",
    "\n",
    "reverse_input_features_dict = dict(\n",
    "    (i, token) for token, i in input_features_dict.items())\n",
    "reverse_target_features_dict = dict(\n",
    "    (i, token) for token, i in target_features_dict.items())\n",
    "\n",
    "\n",
    "max_encoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)) for input_doc in input_docs])\n",
    "max_decoder_seq_length = max([len(re.findall(r\"[\\w']+|[^\\s\\w]\", target_doc)) for target_doc in target_docs])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_docs), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
    "    for timestep, token in enumerate(re.findall(r\"[\\w']+|[^\\s\\w]\", input_doc)):\n",
    "        #Assign 1. for the current line, timestep, & word in encoder_input_data\n",
    "        encoder_input_data[line, timestep, input_features_dict[token]] = 1.\n",
    "    \n",
    "    for timestep, token in enumerate(target_doc.split()):\n",
    "        decoder_input_data[line, timestep, target_features_dict[token]] = 1.\n",
    "        if timestep > 0:\n",
    "            decoder_target_data[line, timestep - 1, target_features_dict[token]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "NPRH_kUNKaHE",
    "outputId": "4fb20057-4685-4332-9dff-45b9de2fc7d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hola', 'buenos d√≠as en que puedo servirle'), ('tienes domicilio', 'claro que si que deseas pedir a domicilio'), ('que cuesta la arepa de huevo', 'la arepa tiene un costo de 2000 pesos sin incluir el domicilio'), ('y vendes chocolatico caliente', 'claro el chocolate un costo de 1000 sin leche'), ('y con leche', 'con lehe tiene un costo de 2000 pesos')]\n",
      "['hola', 'tienes domicilio', 'que cuesta la arepa de huevo', 'y vendes chocolatico caliente', 'y con leche']\n"
     ]
    }
   ],
   "source": [
    "print(pairs[:5])\n",
    "print(input_docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "QZZcikCkFulO",
    "outputId": "d720c441-8b2b-4d6d-87c9-db0e3afe54eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "21/21 [==============================] - 8s 146ms/step - loss: 1.3733 - accuracy: 0.0253 - val_loss: 1.2867 - val_accuracy: 0.0331\n",
      "Epoch 2/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.3091 - accuracy: 0.0344 - val_loss: 1.2758 - val_accuracy: 0.0337\n",
      "Epoch 3/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2177 - accuracy: 0.0346 - val_loss: 1.2793 - val_accuracy: 0.0316\n",
      "Epoch 4/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2592 - accuracy: 0.0336 - val_loss: 1.2799 - val_accuracy: 0.0342\n",
      "Epoch 5/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2099 - accuracy: 0.0358 - val_loss: 1.2838 - val_accuracy: 0.0337\n",
      "Epoch 6/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.1410 - accuracy: 0.0382 - val_loss: 1.2866 - val_accuracy: 0.0347\n",
      "Epoch 7/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1453 - accuracy: 0.0386 - val_loss: 1.2894 - val_accuracy: 0.0357\n",
      "Epoch 8/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1932 - accuracy: 0.0390 - val_loss: 1.3008 - val_accuracy: 0.0367\n",
      "Epoch 9/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2461 - accuracy: 0.0430 - val_loss: 1.2946 - val_accuracy: 0.0352\n",
      "Epoch 10/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1717 - accuracy: 0.0414 - val_loss: 1.2954 - val_accuracy: 0.0367\n",
      "Epoch 11/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.2168 - accuracy: 0.0443 - val_loss: 1.3075 - val_accuracy: 0.0403\n",
      "Epoch 12/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.2081 - accuracy: 0.0459 - val_loss: 1.3093 - val_accuracy: 0.0357\n",
      "Epoch 13/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1453 - accuracy: 0.0429 - val_loss: 1.3008 - val_accuracy: 0.0367\n",
      "Epoch 14/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1405 - accuracy: 0.0491 - val_loss: 1.3049 - val_accuracy: 0.0398\n",
      "Epoch 15/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1310 - accuracy: 0.0469 - val_loss: 1.3020 - val_accuracy: 0.0352\n",
      "Epoch 16/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1335 - accuracy: 0.0461 - val_loss: 1.3041 - val_accuracy: 0.0357\n",
      "Epoch 17/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1984 - accuracy: 0.0447 - val_loss: 1.3078 - val_accuracy: 0.0393\n",
      "Epoch 18/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1165 - accuracy: 0.0492 - val_loss: 1.3099 - val_accuracy: 0.0352\n",
      "Epoch 19/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1612 - accuracy: 0.0485 - val_loss: 1.3162 - val_accuracy: 0.0413\n",
      "Epoch 20/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0620 - accuracy: 0.0510 - val_loss: 1.3242 - val_accuracy: 0.0362\n",
      "Epoch 21/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1246 - accuracy: 0.0512 - val_loss: 1.3147 - val_accuracy: 0.0382\n",
      "Epoch 22/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1364 - accuracy: 0.0509 - val_loss: 1.3304 - val_accuracy: 0.0408\n",
      "Epoch 23/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0961 - accuracy: 0.0519 - val_loss: 1.3189 - val_accuracy: 0.0388\n",
      "Epoch 24/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0725 - accuracy: 0.0579 - val_loss: 1.3177 - val_accuracy: 0.0372\n",
      "Epoch 25/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1264 - accuracy: 0.0555 - val_loss: 1.3184 - val_accuracy: 0.0398\n",
      "Epoch 26/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.1076 - accuracy: 0.0590 - val_loss: 1.3172 - val_accuracy: 0.0362\n",
      "Epoch 27/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0458 - accuracy: 0.0590 - val_loss: 1.3191 - val_accuracy: 0.0423\n",
      "Epoch 28/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0842 - accuracy: 0.0625 - val_loss: 1.3257 - val_accuracy: 0.0423\n",
      "Epoch 29/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0492 - accuracy: 0.0626 - val_loss: 1.3161 - val_accuracy: 0.0423\n",
      "Epoch 30/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0517 - accuracy: 0.0680 - val_loss: 1.3290 - val_accuracy: 0.0474\n",
      "Epoch 31/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 1.0723 - accuracy: 0.0682 - val_loss: 1.3260 - val_accuracy: 0.0505\n",
      "Epoch 32/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0007 - accuracy: 0.0737 - val_loss: 1.3151 - val_accuracy: 0.0479\n",
      "Epoch 33/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0321 - accuracy: 0.0760 - val_loss: 1.3199 - val_accuracy: 0.0556\n",
      "Epoch 34/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0316 - accuracy: 0.0721 - val_loss: 1.3185 - val_accuracy: 0.0510\n",
      "Epoch 35/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0030 - accuracy: 0.0740 - val_loss: 1.3138 - val_accuracy: 0.0479\n",
      "Epoch 36/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 1.0052 - accuracy: 0.0726 - val_loss: 1.3175 - val_accuracy: 0.0571\n",
      "Epoch 37/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.9477 - accuracy: 0.0765 - val_loss: 1.3171 - val_accuracy: 0.0490\n",
      "Epoch 38/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.9737 - accuracy: 0.0815 - val_loss: 1.3265 - val_accuracy: 0.0490\n",
      "Epoch 39/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 1.0598 - accuracy: 0.0863 - val_loss: 1.3088 - val_accuracy: 0.0566\n",
      "Epoch 40/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.9935 - accuracy: 0.0803 - val_loss: 1.3263 - val_accuracy: 0.0541\n",
      "Epoch 41/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.9606 - accuracy: 0.0803 - val_loss: 1.2987 - val_accuracy: 0.0541\n",
      "Epoch 42/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.9917 - accuracy: 0.0798 - val_loss: 1.3007 - val_accuracy: 0.0566\n",
      "Epoch 43/600\n",
      "21/21 [==============================] - 1s 62ms/step - loss: 1.0114 - accuracy: 0.0820 - val_loss: 1.2924 - val_accuracy: 0.0612\n",
      "Epoch 44/600\n",
      "21/21 [==============================] - 2s 112ms/step - loss: 0.9624 - accuracy: 0.0883 - val_loss: 1.2993 - val_accuracy: 0.0576\n",
      "Epoch 45/600\n",
      "21/21 [==============================] - 6s 322ms/step - loss: 0.9702 - accuracy: 0.0849 - val_loss: 1.2998 - val_accuracy: 0.0592\n",
      "Epoch 46/600\n",
      "21/21 [==============================] - 1s 60ms/step - loss: 0.9902 - accuracy: 0.0858 - val_loss: 1.2889 - val_accuracy: 0.0637\n",
      "Epoch 47/600\n",
      "21/21 [==============================] - 1s 59ms/step - loss: 0.9743 - accuracy: 0.0881 - val_loss: 1.2905 - val_accuracy: 0.0612\n",
      "Epoch 48/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.9698 - accuracy: 0.0873 - val_loss: 1.2904 - val_accuracy: 0.0576\n",
      "Epoch 49/600\n",
      "21/21 [==============================] - 1s 59ms/step - loss: 0.9178 - accuracy: 0.0899 - val_loss: 1.3015 - val_accuracy: 0.0622\n",
      "Epoch 50/600\n",
      "21/21 [==============================] - 1s 61ms/step - loss: 0.9772 - accuracy: 0.0846 - val_loss: 1.2893 - val_accuracy: 0.0592\n",
      "Epoch 51/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.9233 - accuracy: 0.0919 - val_loss: 1.2858 - val_accuracy: 0.0643\n",
      "Epoch 52/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.8805 - accuracy: 0.0934 - val_loss: 1.2847 - val_accuracy: 0.0607\n",
      "Epoch 53/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.8948 - accuracy: 0.0948 - val_loss: 1.2743 - val_accuracy: 0.0653\n",
      "Epoch 54/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.8460 - accuracy: 0.0912 - val_loss: 1.2752 - val_accuracy: 0.0617\n",
      "Epoch 55/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.8843 - accuracy: 0.0890 - val_loss: 1.3012 - val_accuracy: 0.0607\n",
      "Epoch 56/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.9431 - accuracy: 0.0956 - val_loss: 1.2852 - val_accuracy: 0.0622\n",
      "Epoch 57/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.8516 - accuracy: 0.0931 - val_loss: 1.2628 - val_accuracy: 0.0643\n",
      "Epoch 58/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21/21 [==============================] - 1s 55ms/step - loss: 0.8952 - accuracy: 0.0967 - val_loss: 1.2879 - val_accuracy: 0.0648\n",
      "Epoch 59/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.8417 - accuracy: 0.1048 - val_loss: 1.2643 - val_accuracy: 0.0663\n",
      "Epoch 60/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.9067 - accuracy: 0.0970 - val_loss: 1.2561 - val_accuracy: 0.0699\n",
      "Epoch 61/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.8221 - accuracy: 0.1032 - val_loss: 1.2436 - val_accuracy: 0.0653\n",
      "Epoch 62/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.8529 - accuracy: 0.1043 - val_loss: 1.2618 - val_accuracy: 0.0694\n",
      "Epoch 63/600\n",
      "21/21 [==============================] - 1s 60ms/step - loss: 0.8523 - accuracy: 0.1033 - val_loss: 1.2642 - val_accuracy: 0.0724\n",
      "Epoch 64/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.8010 - accuracy: 0.1116 - val_loss: 1.2675 - val_accuracy: 0.0678\n",
      "Epoch 65/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.8005 - accuracy: 0.1084 - val_loss: 1.2639 - val_accuracy: 0.0694\n",
      "Epoch 66/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.7911 - accuracy: 0.1062 - val_loss: 1.2677 - val_accuracy: 0.0694\n",
      "Epoch 67/600\n",
      "21/21 [==============================] - 1s 59ms/step - loss: 0.8231 - accuracy: 0.1045 - val_loss: 1.2619 - val_accuracy: 0.0699\n",
      "Epoch 68/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.7268 - accuracy: 0.1129 - val_loss: 1.2815 - val_accuracy: 0.0622\n",
      "Epoch 69/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.7811 - accuracy: 0.1132 - val_loss: 1.2938 - val_accuracy: 0.0637\n",
      "Epoch 70/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.7790 - accuracy: 0.1075 - val_loss: 1.2542 - val_accuracy: 0.0745\n",
      "Epoch 71/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.7489 - accuracy: 0.1176 - val_loss: 1.2574 - val_accuracy: 0.0719\n",
      "Epoch 72/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.7534 - accuracy: 0.1186 - val_loss: 1.2429 - val_accuracy: 0.0755\n",
      "Epoch 73/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.7384 - accuracy: 0.1131 - val_loss: 1.2512 - val_accuracy: 0.0780\n",
      "Epoch 74/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.7644 - accuracy: 0.1202 - val_loss: 1.2337 - val_accuracy: 0.0734\n",
      "Epoch 75/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.7353 - accuracy: 0.1153 - val_loss: 1.2479 - val_accuracy: 0.0775\n",
      "Epoch 76/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.7293 - accuracy: 0.1287 - val_loss: 1.2254 - val_accuracy: 0.0775\n",
      "Epoch 77/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.7227 - accuracy: 0.1252 - val_loss: 1.2363 - val_accuracy: 0.0780\n",
      "Epoch 78/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.7279 - accuracy: 0.1264 - val_loss: 1.2452 - val_accuracy: 0.0796\n",
      "Epoch 79/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.7103 - accuracy: 0.1286 - val_loss: 1.2322 - val_accuracy: 0.0770\n",
      "Epoch 80/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.7037 - accuracy: 0.1232 - val_loss: 1.2317 - val_accuracy: 0.0816\n",
      "Epoch 81/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.7128 - accuracy: 0.1336 - val_loss: 1.2604 - val_accuracy: 0.0785\n",
      "Epoch 82/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.7176 - accuracy: 0.1309 - val_loss: 1.3058 - val_accuracy: 0.0678\n",
      "Epoch 83/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.8142 - accuracy: 0.1254 - val_loss: 1.2419 - val_accuracy: 0.0790\n",
      "Epoch 84/600\n",
      "21/21 [==============================] - 1s 60ms/step - loss: 0.7007 - accuracy: 0.1285 - val_loss: 1.2427 - val_accuracy: 0.0816\n",
      "Epoch 85/600\n",
      "21/21 [==============================] - 1s 61ms/step - loss: 0.6831 - accuracy: 0.1385 - val_loss: 1.2373 - val_accuracy: 0.0811\n",
      "Epoch 86/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.7382 - accuracy: 0.1378 - val_loss: 1.2441 - val_accuracy: 0.0775\n",
      "Epoch 87/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6704 - accuracy: 0.1357 - val_loss: 1.2289 - val_accuracy: 0.0811\n",
      "Epoch 88/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.6816 - accuracy: 0.1425 - val_loss: 1.2285 - val_accuracy: 0.0831\n",
      "Epoch 89/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6686 - accuracy: 0.1402 - val_loss: 1.2441 - val_accuracy: 0.0826\n",
      "Epoch 90/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6889 - accuracy: 0.1385 - val_loss: 1.2455 - val_accuracy: 0.0836\n",
      "Epoch 91/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.6196 - accuracy: 0.1464 - val_loss: 1.2293 - val_accuracy: 0.0816\n",
      "Epoch 92/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6476 - accuracy: 0.1468 - val_loss: 1.2311 - val_accuracy: 0.0816\n",
      "Epoch 93/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6508 - accuracy: 0.1455 - val_loss: 1.2386 - val_accuracy: 0.0826\n",
      "Epoch 94/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6480 - accuracy: 0.1474 - val_loss: 1.2254 - val_accuracy: 0.0857\n",
      "Epoch 95/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.6809 - accuracy: 0.1470 - val_loss: 1.2297 - val_accuracy: 0.0765\n",
      "Epoch 96/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.6474 - accuracy: 0.1474 - val_loss: 1.2452 - val_accuracy: 0.0801\n",
      "Epoch 97/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6667 - accuracy: 0.1531 - val_loss: 1.2217 - val_accuracy: 0.0867\n",
      "Epoch 98/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.5866 - accuracy: 0.1527 - val_loss: 1.2443 - val_accuracy: 0.0801\n",
      "Epoch 99/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6231 - accuracy: 0.1568 - val_loss: 1.2590 - val_accuracy: 0.0775\n",
      "Epoch 100/600\n",
      "21/21 [==============================] - ETA: 0s - loss: 0.6587 - accuracy: 0.15 - 1s 59ms/step - loss: 0.6574 - accuracy: 0.1552 - val_loss: 1.2276 - val_accuracy: 0.0872\n",
      "Epoch 101/600\n",
      "21/21 [==============================] - 1s 64ms/step - loss: 0.5896 - accuracy: 0.1549 - val_loss: 1.2179 - val_accuracy: 0.0836\n",
      "Epoch 102/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.6463 - accuracy: 0.1587 - val_loss: 1.2256 - val_accuracy: 0.0852\n",
      "Epoch 103/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6062 - accuracy: 0.1576 - val_loss: 1.2228 - val_accuracy: 0.0831\n",
      "Epoch 104/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6341 - accuracy: 0.1671 - val_loss: 1.2423 - val_accuracy: 0.0852\n",
      "Epoch 105/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.6125 - accuracy: 0.1589 - val_loss: 1.2137 - val_accuracy: 0.0877\n",
      "Epoch 106/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.5504 - accuracy: 0.1570 - val_loss: 1.2216 - val_accuracy: 0.0882\n",
      "Epoch 107/600\n",
      "21/21 [==============================] - 1s 60ms/step - loss: 0.6043 - accuracy: 0.1640 - val_loss: 1.2276 - val_accuracy: 0.0892\n",
      "Epoch 108/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.6390 - accuracy: 0.1643 - val_loss: 1.2280 - val_accuracy: 0.0852\n",
      "Epoch 109/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5161 - accuracy: 0.1589 - val_loss: 1.2339 - val_accuracy: 0.0821\n",
      "Epoch 110/600\n",
      "21/21 [==============================] - 1s 61ms/step - loss: 0.6087 - accuracy: 0.1716 - val_loss: 1.2234 - val_accuracy: 0.0857\n",
      "Epoch 111/600\n",
      "21/21 [==============================] - 1s 67ms/step - loss: 0.5674 - accuracy: 0.1740 - val_loss: 1.2298 - val_accuracy: 0.0852\n",
      "Epoch 112/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.5968 - accuracy: 0.1678 - val_loss: 1.2272 - val_accuracy: 0.0852\n",
      "Epoch 113/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5690 - accuracy: 0.1662 - val_loss: 1.2187 - val_accuracy: 0.0872\n",
      "Epoch 114/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5458 - accuracy: 0.1664 - val_loss: 1.2193 - val_accuracy: 0.0867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.5775 - accuracy: 0.1662 - val_loss: 1.2318 - val_accuracy: 0.0831\n",
      "Epoch 116/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.6099 - accuracy: 0.1720 - val_loss: 1.2096 - val_accuracy: 0.0913\n",
      "Epoch 117/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.5374 - accuracy: 0.1787 - val_loss: 1.2944 - val_accuracy: 0.0745\n",
      "Epoch 118/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.6337 - accuracy: 0.1675 - val_loss: 1.2273 - val_accuracy: 0.0831\n",
      "Epoch 119/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.5343 - accuracy: 0.1692 - val_loss: 1.2268 - val_accuracy: 0.0847\n",
      "Epoch 120/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.5284 - accuracy: 0.1751 - val_loss: 1.2284 - val_accuracy: 0.0867\n",
      "Epoch 121/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.5121 - accuracy: 0.1769 - val_loss: 1.2092 - val_accuracy: 0.0877\n",
      "Epoch 122/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.5823 - accuracy: 0.1804 - val_loss: 1.2322 - val_accuracy: 0.0847\n",
      "Epoch 123/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5243 - accuracy: 0.1756 - val_loss: 1.2123 - val_accuracy: 0.0882\n",
      "Epoch 124/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5444 - accuracy: 0.1754 - val_loss: 1.2306 - val_accuracy: 0.0852\n",
      "Epoch 125/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.5454 - accuracy: 0.1696 - val_loss: 1.2322 - val_accuracy: 0.0852\n",
      "Epoch 126/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5383 - accuracy: 0.1794 - val_loss: 1.2261 - val_accuracy: 0.0852\n",
      "Epoch 127/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4967 - accuracy: 0.1748 - val_loss: 1.2326 - val_accuracy: 0.0826\n",
      "Epoch 128/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5136 - accuracy: 0.1745 - val_loss: 1.2206 - val_accuracy: 0.0887\n",
      "Epoch 129/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.5102 - accuracy: 0.1771 - val_loss: 1.2174 - val_accuracy: 0.0908\n",
      "Epoch 130/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4936 - accuracy: 0.1716 - val_loss: 1.2173 - val_accuracy: 0.0862\n",
      "Epoch 131/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5225 - accuracy: 0.1725 - val_loss: 1.2249 - val_accuracy: 0.0867\n",
      "Epoch 132/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.4912 - accuracy: 0.1823 - val_loss: 1.2092 - val_accuracy: 0.0862\n",
      "Epoch 133/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4651 - accuracy: 0.1823 - val_loss: 1.2181 - val_accuracy: 0.0882\n",
      "Epoch 134/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5176 - accuracy: 0.1928 - val_loss: 1.2210 - val_accuracy: 0.0857\n",
      "Epoch 135/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4786 - accuracy: 0.1835 - val_loss: 1.2300 - val_accuracy: 0.0867\n",
      "Epoch 136/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4697 - accuracy: 0.1804 - val_loss: 1.2482 - val_accuracy: 0.0857\n",
      "Epoch 137/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4836 - accuracy: 0.1846 - val_loss: 1.2382 - val_accuracy: 0.0857\n",
      "Epoch 138/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.4941 - accuracy: 0.1821 - val_loss: 1.2121 - val_accuracy: 0.0877\n",
      "Epoch 139/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.4773 - accuracy: 0.1843 - val_loss: 1.2293 - val_accuracy: 0.0852\n",
      "Epoch 140/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.4603 - accuracy: 0.1834 - val_loss: 1.2300 - val_accuracy: 0.0821\n",
      "Epoch 141/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.5141 - accuracy: 0.1907 - val_loss: 1.2157 - val_accuracy: 0.0862\n",
      "Epoch 142/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.5208 - accuracy: 0.1880 - val_loss: 1.2361 - val_accuracy: 0.0847\n",
      "Epoch 143/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4384 - accuracy: 0.1771 - val_loss: 1.2545 - val_accuracy: 0.0816\n",
      "Epoch 144/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4745 - accuracy: 0.1866 - val_loss: 1.2124 - val_accuracy: 0.0852\n",
      "Epoch 145/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4755 - accuracy: 0.1932 - val_loss: 1.2196 - val_accuracy: 0.0836\n",
      "Epoch 146/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4826 - accuracy: 0.1808 - val_loss: 1.2224 - val_accuracy: 0.0852\n",
      "Epoch 147/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4702 - accuracy: 0.1873 - val_loss: 1.2047 - val_accuracy: 0.0892\n",
      "Epoch 148/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.4682 - accuracy: 0.1856 - val_loss: 1.2285 - val_accuracy: 0.0847\n",
      "Epoch 149/600\n",
      "21/21 [==============================] - 1s 65ms/step - loss: 0.4733 - accuracy: 0.1967 - val_loss: 1.2206 - val_accuracy: 0.0877\n",
      "Epoch 150/600\n",
      "21/21 [==============================] - 1s 69ms/step - loss: 0.4815 - accuracy: 0.1909 - val_loss: 1.2252 - val_accuracy: 0.0857\n",
      "Epoch 151/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.4323 - accuracy: 0.1881 - val_loss: 1.2162 - val_accuracy: 0.0882\n",
      "Epoch 152/600\n",
      "21/21 [==============================] - 1s 59ms/step - loss: 0.4280 - accuracy: 0.1861 - val_loss: 1.2247 - val_accuracy: 0.0882\n",
      "Epoch 153/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.4249 - accuracy: 0.1890 - val_loss: 1.2185 - val_accuracy: 0.0852\n",
      "Epoch 154/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.4577 - accuracy: 0.1920 - val_loss: 1.2153 - val_accuracy: 0.0903\n",
      "Epoch 155/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4027 - accuracy: 0.1942 - val_loss: 1.2557 - val_accuracy: 0.0831\n",
      "Epoch 156/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.5169 - accuracy: 0.2139 - val_loss: 1.2296 - val_accuracy: 0.0831\n",
      "Epoch 157/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.4361 - accuracy: 0.1920 - val_loss: 1.2292 - val_accuracy: 0.0857\n",
      "Epoch 158/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.4647 - accuracy: 0.1991 - val_loss: 1.2204 - val_accuracy: 0.0877\n",
      "Epoch 159/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.4877 - accuracy: 0.2055 - val_loss: 1.2186 - val_accuracy: 0.0862\n",
      "Epoch 160/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4459 - accuracy: 0.1955 - val_loss: 1.2329 - val_accuracy: 0.0857\n",
      "Epoch 161/600\n",
      "21/21 [==============================] - 1s 64ms/step - loss: 0.3985 - accuracy: 0.1943 - val_loss: 1.2170 - val_accuracy: 0.0898\n",
      "Epoch 162/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.4023 - accuracy: 0.1959 - val_loss: 1.2222 - val_accuracy: 0.0867\n",
      "Epoch 163/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.4347 - accuracy: 0.1973 - val_loss: 1.2100 - val_accuracy: 0.0862\n",
      "Epoch 164/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.4387 - accuracy: 0.1993 - val_loss: 1.2295 - val_accuracy: 0.0872\n",
      "Epoch 165/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.4374 - accuracy: 0.2031 - val_loss: 1.2115 - val_accuracy: 0.0887\n",
      "Epoch 166/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4282 - accuracy: 0.1924 - val_loss: 1.2153 - val_accuracy: 0.0852\n",
      "Epoch 167/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4092 - accuracy: 0.1980 - val_loss: 1.2334 - val_accuracy: 0.0836\n",
      "Epoch 168/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.4663 - accuracy: 0.2131 - val_loss: 1.2072 - val_accuracy: 0.0877\n",
      "Epoch 169/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.3603 - accuracy: 0.1949 - val_loss: 1.2410 - val_accuracy: 0.0826\n",
      "Epoch 170/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4397 - accuracy: 0.2022 - val_loss: 1.2114 - val_accuracy: 0.0887\n",
      "Epoch 171/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4127 - accuracy: 0.1962 - val_loss: 1.2293 - val_accuracy: 0.0857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 172/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4151 - accuracy: 0.1960 - val_loss: 1.2403 - val_accuracy: 0.0841\n",
      "Epoch 173/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.3677 - accuracy: 0.1888 - val_loss: 1.2283 - val_accuracy: 0.0887\n",
      "Epoch 174/600\n",
      "21/21 [==============================] - 1s 55ms/step - loss: 0.3640 - accuracy: 0.1883 - val_loss: 1.2264 - val_accuracy: 0.0847\n",
      "Epoch 175/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.3906 - accuracy: 0.1992 - val_loss: 1.2302 - val_accuracy: 0.0831\n",
      "Epoch 176/600\n",
      "21/21 [==============================] - 1s 56ms/step - loss: 0.4431 - accuracy: 0.2047 - val_loss: 1.2303 - val_accuracy: 0.0816\n",
      "Epoch 177/600\n",
      "21/21 [==============================] - 1s 57ms/step - loss: 0.3855 - accuracy: 0.2076 - val_loss: 1.2426 - val_accuracy: 0.0816\n",
      "Epoch 178/600\n",
      "21/21 [==============================] - 1s 58ms/step - loss: 0.3935 - accuracy: 0.2038 - val_loss: 1.2267 - val_accuracy: 0.0852\n",
      "Epoch 179/600\n",
      "11/21 [==============>...............] - ETA: 0s - loss: 0.3320 - accuracy: 0.1851"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "#Dimensionality\n",
    "dimensionality = 256\n",
    "#The batch size and number of epochs\n",
    "batch_size = 10\n",
    "epochs = 600\n",
    "#Encoder\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder_lstm = LSTM(dimensionality, return_state=True)\n",
    "encoder_outputs, state_hidden, state_cell = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [state_hidden, state_cell]\n",
    "#Decoder\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "decoder_lstm = LSTM(dimensionality, return_sequences=True, return_state=True)\n",
    "decoder_outputs, decoder_state_hidden, decoder_state_cell = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "#Model\n",
    "training_model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "#Compiling\n",
    "training_model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'], sample_weight_mode='temporal')\n",
    "#Training\n",
    "training_model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size = batch_size, epochs = epochs, validation_split = 0.2)\n",
    "training_model.save('model/training_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ST5NJzSwJlDB"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "training_model = load_model('model/training_model.h5')\n",
    "encoder_inputs = training_model.input[0]\n",
    "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "latent_dim = 256\n",
    "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
    "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
    "decoder_outputs, state_hidden, state_cell = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_hidden, state_cell]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "def decode_response(test_input):\n",
    "    #Getting the output states to pass into the decoder\n",
    "    states_value = encoder_model.predict(test_input)\n",
    "    #Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    #Setting the first token of target sequence with the start token\n",
    "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
    "    \n",
    "    #A variable to store our response word by word\n",
    "    decoded_sentence = ''\n",
    "    \n",
    "    stop_condition = False\n",
    "while not stop_condition:\n",
    "      #Predicting output tokens with probabilities and states\n",
    "      output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
    "#Choosing the one with highest probability\n",
    "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "      sampled_token = reverse_target_features_dict[sampled_token_index]\n",
    "      decoded_sentence += \" \" + sampled_token\n",
    "#Stop if hit max length or found the stop token\n",
    "      if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
    "        stop_condition = True\n",
    "#Update the target sequence\n",
    "      target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "      target_seq[0, 0, sampled_token_index] = 1.\n",
    "      #Update states\n",
    "      states_value = [hidden_state, cell_state]\n",
    "return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "0JTKCjKVJ92i",
    "outputId": "f913afe7-5cc9-4fc5-f631-03ff762719be"
   },
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "  negative_responses = (\"no\", \"negativo\", \"nada\", \"nop\", \"nopi\")\n",
    "  exit_commands = (\"chao\", \"adios\", \"nos vemos\",\n",
    "                 \"suerte\", \"chaito\", \"bye\", \"chaolin\")\n",
    "#Method to start the conversation\n",
    "  def start_chat(self):\n",
    "    user_response = input(\"Hola, bienvenido en que te puedo ayudar?\\n\")\n",
    "    \n",
    "    if user_response in self.negative_responses:\n",
    "      print(\"Ok, perfecto, que tengas un hermoso d√≠a!\")\n",
    "      return\n",
    "    self.chat(user_response)\n",
    "#Method to handle the conversation\n",
    "  def chat(self, reply):\n",
    "    while not self.make_exit(reply):\n",
    "      reply = input(self.generate_response(reply)+\"\\n\")\n",
    "    \n",
    "  #Method to convert user input into a matrix\n",
    "  def string_to_matrix(self, user_input):\n",
    "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
    "    user_input_matrix = np.zeros(\n",
    "      (1, max_encoder_seq_length, num_encoder_tokens),\n",
    "      dtype='float32')\n",
    "    for timestep, token in enumerate(tokens):\n",
    "      if token in input_features_dict:\n",
    "        user_input_matrix[0, timestep, input_features_dict[token]] = 1.\n",
    "    return user_input_matrix\n",
    "  \n",
    "  #Method that will create a response using seq2seq model we built\n",
    "  def generate_response(self, user_input):\n",
    "    input_matrix = self.string_to_matrix(user_input)\n",
    "    chatbot_response = decode_response(input_matrix)\n",
    "    #Remove <START> and <END> tokens from chatbot_response\n",
    "    chatbot_response = chatbot_response.replace(\"<START>\",'')\n",
    "    chatbot_response = chatbot_response.replace(\"<END>\",'')\n",
    "    return chatbot_response\n",
    "#Method to check for exit commands\n",
    "  def make_exit(self, reply):\n",
    "    for exit_command in self.exit_commands:\n",
    "      if exit_command in reply:\n",
    "        print(\"Que tengas un hermoso d√≠a!\")\n",
    "        return True\n",
    "    return False\n",
    "  \n",
    "chatbot = ChatBot()\n",
    "chatbot.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "C7J5OMxJKEDK",
    "outputId": "008db9fd-b885-4bd6-ebb0-d86cd01d8825"
   },
   "outputs": [],
   "source": [
    "chatbot.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JN9jd7lQqtbq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
